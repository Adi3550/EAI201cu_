# TASK 3 â€” MODEL TRAINING & EVALUATION

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

sns.set(style="whitegrid")

assert "Class_Type" in df.columns, "Target column 'Class_Type' not found"

if "animal_name" in df.columns:
    df = df.drop(columns=["animal_name"])

TARGET = "Class_Type"
y = df[TARGET].astype(str)
X = df.drop(columns=[TARGET])

engineered_feature_names = [
    c for c in ["habitat_risk_score", "trophic_level"] if c in X.columns
]

numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = ["habitat_type", "diet", "conservation_status"]

print("Numeric cols:", numeric_cols)
print("Categorical cols:", cat_cols)
print("Engineered features:", engineered_feature_names)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, train_size=0.75, test_size=0.25, random_state=123, stratify=y
)

# Preprocessors
preprocessor_rf = ColumnTransformer(
    transformers=[
        ("num", "passthrough", numeric_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
    ]
)

preprocessor_knn = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), cat_cols),
    ]
)

# Random Forest
rf_clf = RandomForestClassifier(
    n_estimators=80, max_depth=8, min_samples_split=6,
    random_state=123, n_jobs=-1
)

rf_pipeline = Pipeline([("preproc", preprocessor_rf), ("clf", rf_clf)])
rf_pipeline.fit(X_train, y_train)

# KNN
knn_clf = KNeighborsClassifier(n_neighbors=5)
knn_pipeline = Pipeline([("preproc", preprocessor_knn), ("clf", knn_clf)])
knn_pipeline.fit(X_train, y_train)

# Performance
y_train_pred_rf = rf_pipeline.predict(X_train)
y_test_pred_rf = rf_pipeline.predict(X_test)

train_accuracy = accuracy_score(y_train, y_train_pred_rf)
test_accuracy = accuracy_score(y_test, y_test_pred_rf)
overfit_gap = train_accuracy - test_accuracy

print(f"Random Forest training accuracy: {train_accuracy:.4f}")
print(f"Random Forest testing accuracy : {test_accuracy:.4f}")
print(f"Overfitting gap                : {overfit_gap:.4f}")

y_test_pred_knn = knn_pipeline.predict(X_test)
knn_test_accuracy = accuracy_score(y_test, y_test_pred_knn)
print(f"\nKNN (k=5) testing accuracy     : {knn_test_accuracy:.4f}")

print("\n--- Random Forest Classification Report ---")
display(pd.DataFrame(classification_report(
    y_test, y_test_pred_rf, output_dict=True)).T)

print("\n--- KNN Classification Report ---")
display(pd.DataFrame(classification_report(
    y_test, y_test_pred_knn, output_dict=True)).T)

# Confusion Matrix
cm = confusion_matrix(y_test, y_test_pred_rf,
                      labels=rf_pipeline.named_steps["clf"].classes_)
cm_df = pd.DataFrame(
    cm,
    index=rf_pipeline.named_steps["clf"].classes_,
    columns=rf_pipeline.named_steps["clf"].classes_
)

plt.figure(figsize=(8,6))
sns.heatmap(cm_df, annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Random Forest Confusion Matrix")
plt.show()

# Feature Importance
preprocessor_rf.fit(X_train)
num_names = numeric_cols
cat_names = list(preprocessor_rf.named_transformers_["cat"]
                 .get_feature_names_out(cat_cols))
feature_names = num_names + cat_names

importances = rf_pipeline.named_steps["clf"].feature_importances_
feat_imp = pd.Series(importances, index=feature_names).sort_values(ascending=False)

top_feat = feat_imp.head(12)
colors = ["#d62728" if f in engineered_feature_names else "#1f77b4"
          for f in top_feat.index]

plt.figure(figsize=(9,6))
top_feat.sort_values().plot(kind="barh", color=colors)
plt.title("Top 12 Feature Importances (Random Forest)")
plt.xlabel("Importance")

import matplotlib.patches as mpatches
plt.legend(handles=[
    mpatches.Patch(color="#d62728", label="Engineered Features"),
    mpatches.Patch(color="#1f77b4", label="Original Features")
])
plt.show()

# Comparison Table
results_df = pd.DataFrame({
    "model": ["RandomForest", "KNN"],
    "test_accuracy": [test_accuracy, knn_test_accuracy]
}).set_index("model")

print("\nModel comparison (test accuracy):")
display(results_df)

# CRITICAL ANALYSIS
print("\n===== MODEL CRITICAL ANALYSIS =====")

rf_report = classification_report(y_test, y_test_pred_rf, output_dict=True)
rf_report_df = pd.DataFrame(rf_report).T

actual_classes = sorted(y_test.unique().tolist())
class_f1_scores = rf_report_df.loc[actual_classes, "f1-score"]

top_feature = feat_imp.idxmax()
print(f"1. Most important feature: {top_feature} (importance = {feat_imp.max():.3f})")

worst_class = class_f1_scores.idxmin()
print(f"2. Worst performing class: {worst_class} (F1 = {class_f1_scores.min():.3f})")

best_class = class_f1_scores.idxmax()
print(f"3. Best performing class: {best_class} (F1 = {class_f1_scores.max():.3f})")

for feat in engineered_feature_names:
    if feat in feat_imp.index:
        rank = int(feat_imp.rank(ascending=False)[feat])
        print(f"4. Engineered feature '{feat}' ranked #{rank}")
    else:
        print(f"4. Engineered feature '{feat}' not found")

print(f"5. Model comparison summary: KNN = {knn_test_accuracy:.3f} vs RF = {test_accuracy:.3f}")

print("===== END OF CRITICAL ANALYSIS =====")
